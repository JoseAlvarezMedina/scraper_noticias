name: Scraper CI

on:
  schedule:
    - cron: '0 */6 * * *'  # ‚è±Ô∏è Ejecuta cada 6 horas
  workflow_dispatch:       # üöÄ Permite ejecuci√≥n manual desde GitHub

jobs:
  lint-and-test:
    name: Lint & Test
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies + pytest
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run pytest
        run: |
          export PYTHONPATH=.
          pytest --maxfail=1 --disable-warnings -q

  scrape-and-push:
    name: Scrape & Publish Artifacts
    needs: lint-and-test
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Scrapy spider
        run: scrapy crawl titulares -s LOG_LEVEL=INFO

      - name: Upload CSV & DB as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: noticias-data
          path: |
            noticias.csv
            noticias.db

