
```markdown
# ğŸ“° Scraper de Noticias Colombianas con AnÃ¡lisis de Sentimiento

Este proyecto automatiza la recolecciÃ³n de titulares de medios colombianos usando **Scrapy**, los almacena en formatos CSV y SQLite, y aplica anÃ¡lisis de sentimiento con **VADER**. EstÃ¡ pensado para ejecutarse cada 6 horas mediante **GitHub Actions**.

---

## ğŸ“Œ Objetivo

Recolectar y analizar noticias de medios regionales de Colombia para alimentar sistemas de monitoreo, anÃ¡lisis de tendencias o dashboards de comunicaciÃ³n.

---

## ğŸ•¸ï¸ Medios cubiertos

- La RepÃºblica (`https://www.larepublica.co/rss`)
- CrÃ³nica del QuindÃ­o (`https://www.cronicadelquindio.com/rss`)
- El PilÃ³n (`https://www.elpilon.com.co/rss`)
- La Silla VacÃ­a (`https://lasillavacia.com/feed`)
- Eje 21 (`https://www.eje21.com.co/rss2`)

---

## ğŸ§  Funcionalidades

- Scraping vÃ­a feeds RSS
- Limpieza y normalizaciÃ³n de datos
- AnÃ¡lisis de sentimiento con VADER (positivo, negativo, neutral)
- Persistencia en:
  - `noticias.csv` (acumulativo, sin duplicados)
  - `noticias.db` (SQLite, clave primaria: URL)
- AutomatizaciÃ³n cada 6 horas vÃ­a GitHub Actions

---

## ğŸ“ Estructura del repositorio

```

noticias\_Scraper/
â”‚
â”œâ”€â”€ noticias/
â”‚   â”œâ”€â”€ spiders/              # Spider principal: titulares.py
â”‚   â”œâ”€â”€ pipelines.py          # Limpieza, sentimiento, persistencia
â”‚   â””â”€â”€ items.py              # DefiniciÃ³n de campos
â”‚
â”œâ”€â”€ tests/                    # Pruebas unitarias (pytest)
â”‚   â”œâ”€â”€ test\_clean.py
â”‚   â”œâ”€â”€ test\_sentiment.py
â”‚   â””â”€â”€ test\_spider.py
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ scraper.yml           # GitHub Actions: ejecuta cada 6h
â”‚
â”œâ”€â”€ requirements.txt          # Dependencias
â””â”€â”€ README.md                 # Este archivo

````

---

## âš™ï¸ InstalaciÃ³n local

```bash
git clone https://github.com/tu-usuario/scraper_noticias.git
cd scraper_noticias
python -m venv venv
source venv/bin/activate  # o venv\Scripts\activate en Windows
pip install -r requirements.txt
````

---

## â–¶ï¸ EjecuciÃ³n manual

```bash
scrapy crawl titulares
```

Los resultados se guardan en `noticias.csv` y `noticias.db`.

---

## âœ… Pruebas

```bash
pytest
```

Incluye validaciÃ³n de limpieza, anÃ¡lisis de sentimiento y comportamiento del spider.

---

## ğŸ”„ AutomatizaciÃ³n con GitHub Actions

El flujo de trabajo se ejecuta automÃ¡ticamente cada 6 horas:

* Lint (opcional)
* Test con `pytest`
* EjecuciÃ³n del spider
* Subida de artefactos (`noticias.csv`, `noticias.db`)

Puedes forzar la ejecuciÃ³n manual desde la pestaÃ±a "Actions".

---

## ğŸ“¦ Requisitos principales

* Python 3.11+
* Scrapy
* vaderSentiment
* pytest

---

## ğŸ“¬ Contacto

Desarrollado por [JosÃ© Ãlvarez](https://www.linkedin.com/in/jose-alvarez-dev/)
Licencia: MIT

```

